from imbami.utils.base_mitigation_method import BaseMitigationMethod
import pandas as pd
import numpy as np

class cSMOGN(BaseMitigationMethod):
    def __init__(self,
                data: pd.DataFrame,
                target_column: str,
                relevance_values: pd.Series):
        '''
        data : pd.Dataframe
            Dataframe containing the data to be sampled.
        target_column : str
            Name of the target variable.
        relevance_values : pd.Series
            Series containing the respective relevance values to the samples/rows in data.
        '''
        super().__init__(data, target_column, relevance_values)
        if not ((relevance_values >= 0) & (relevance_values <= 1)).all():
            out_of_bounds = relevance_values[(relevance_values < 0) | (relevance_values > 1)]
            raise ValueError(f"Relevance contains values out of bounds [0, 1]:\n{out_of_bounds}")

    def run_sampling(self,
                    oversample_rate: float = 0.5,
                    undersample_rate: float = 0.5,
                    knns: int = 5,
                    num_bins: int = 10,
                    allowed_bin_deviation: int = 1,
                    noise_factor: float = 0.01,
                    ignore_categorical_similarity: bool = False,
                    enable_undersampling: bool = True) -> pd.DataFrame:
        """
        Applies cSMOGN to the given dataset,
        performing both oversampling and undersampling to handle imbalanced regression tasks.

        Parameters:
        ----------        
        oversample_rate : float, optional
            Rate with which to oversample the dataset. It defines the number of samples to be created. 
            For example, if the rate is 0.5 and the dataset contains 100 samples, 50 samples will be created. Default is 0.5.
        
        undersample_rate : float, optional
            Rate with which to oversample the dataset. It defines the number of samples to be dropped.
            For example, if the rate is 0.5 and the dataset contains 100 samples, 50 samples will be dropped. Default is 0.5.
        
        knns : int, optional
            Number of k-nearest neighbors considered for oversampling.

        num_bins : int, optional
            The number of bins to use for discretizing numeric columns. Default is 10.
        
        allowed_bin_deviation : int, optional
            The maximum allowed deviation in binning to consider samples similar. Default is 1.
        
        noise_factor : float, optional
            The multiplier applied to the standard deviations when adding Gaussian noise to numeric columns. Default is 0.1.
        
        ignore_categorical_similarity : bool, optional
            If True, categorical features are ignored when calculating similarity between samples. Default is False.
        
        enable_undersampling : bool, optional
            If True, undersampling is performed on the dataset. Default is True.

        Returns:
        -------
        new_dataset : pd.DataFrame
            The new dataset with the synthetic samples added and undersampling applied (if enabled).
 
        Notes:
        -----
            - num_gaussian_samples: int, the number of samples generated using Gaussian noise.
            - num_knn_samples: int, the number of samples generated through interpolation with nearest neighbors.
            - num_dropped_samples: int, the number of samples dropped due to undersampling.
        """


        self.allowed_bin_deviation = allowed_bin_deviation
        self.ignore_categorical_similarity = ignore_categorical_similarity
        self.noise_factor = noise_factor
        self.num_bins = num_bins
        # Discretize the dataset for similarity calculations
        self.binned_data = self.discretize_dataset(
            self.data, self.num_bins, self.numeric_columns, self.categorical_columns, self.ignore_categorical_similarity).to_numpy()



        # Prepare for oversampling
 
        self.relevance_values_numpy = self.relevance_values.to_numpy()

        total_oversample_count = int(self.data_numpy.shape[0]*oversample_rate)
        oversampled_data = np.zeros((total_oversample_count, self.data_numpy.shape[1]))

        #  Counters for the number of samples generated by each method
        self.num_gaussian_samples = 0
        self.num_interpolated_samples = 0
        self.num_oversampled_samples = 0

        while self.num_oversampled_samples < total_oversample_count:
            random_index = np.random.randint(0, self.relevance_values_numpy.shape[0])
            sample_weight = self.relevance_values_numpy[random_index]
            sample_weight = sample_weight**2
            if sample_weight > np.random.uniform(low=0, high=1):
                # rare samples have a high weight, thus over-sample whenever larger then random


                # Generate synthetic samples using interpolation and Gaussian noise
                new_sample, interpolated = self.oversample(reference_sample_index= random_index,
                                                                    knns=knns)

                # Store the new samples
                oversampled_data[self.num_oversampled_samples:self.num_oversampled_samples + 1, :] = new_sample
                self.num_oversampled_samples += 1
                if interpolated:
                    self.num_interpolated_samples += 1
                else:
                    self.num_gaussian_samples += 1
                


        # Postprocessing. Back to Pandas.
        oversampled_data = pd.DataFrame(data = oversampled_data, columns= self.data.columns, index= range(self.data.index.max(), self.data.index.max() + oversampled_data.shape[0]))
        oversampled_data = oversampled_data.astype(self.data.dtypes)
        
        self.num_undersampled_samples = 0
        if enable_undersampling:
            n_undersample = int(self.data_numpy.shape[0] * undersample_rate)
            undersample_indice = -np.ones(n_undersample, dtype=int)
            while self.num_undersampled_samples < n_undersample:
                # pick random index and the respective inverted weight
                random_index = np.random.randint(0, self.relevance_values_numpy.shape[0])
                sample_weight = 1 - self.relevance_values_numpy[random_index]
                sample_weight = sample_weight**2
                if sample_weight > np.random.uniform(low=0, high=1):
                    if random_index not in undersample_indice:
                        undersample_indice[self.num_undersampled_samples] = random_index
                        self.num_undersampled_samples += 1
            

            undersampled_data = self.data.drop(index = self.data.iloc[undersample_indice].index.tolist())
            new_dataset = pd.concat([undersampled_data, oversampled_data], axis=0)
        else:
            new_dataset = pd.concat([self.data, oversampled_data], axis = 0)


        return new_dataset





    def oversample(self, reference_sample_index: int, 
                    knns: int) -> tuple[np.ndarray, bool]:
        # get similar samples indice (rows) based on the discretized data
        similar_rows = self.get_similar_samples(
                index = reference_sample_index, 
                binned_data = self.binned_data, 
                numerical_mask =self.numerical_mask, 
                categorical_mask =self.categorical_mask, 
                allowed_bin_deviation = self.allowed_bin_deviation, 
                ignore_categoricals = self.ignore_categorical_similarity
            ) 
        # check if similar samples where found. If yes, interpolate new sample, if not, use gaussian noise
        if similar_rows.shape[0] > 0:
            # get the samples to the corresponding indices
            similar_samples = self.data_numpy[similar_rows]

            # check if enough similar samples where found. If yes, get k-nearest neighbors.
            if similar_samples.shape[0] > knns:
                distances = self.heom_distance(x=self.data_numpy[reference_sample_index],
                                    y=similar_samples,
                                    feature_ranges= self.feature_ranges,
                                    categorical_mask=self.categorical_mask,
                                    numerical_mask=self.numerical_mask)
                nearest_neighbors = similar_samples[np.argsort(distances)[:knns]]

            else:
                nearest_neighbors = similar_samples

            # randomly pick one of the samples for interpolation
            neighbor_sample = nearest_neighbors[np.random.choice(nearest_neighbors.shape[0], replace=False)]
            new_sample = self.interpolate_sample(x=self.data_numpy[reference_sample_index],
                                                                    y=neighbor_sample,
                                                                    feature_ranges= self.feature_ranges,
                                                                    categorical_mask= self.categorical_mask,
                                                                    numerical_mask=self.numerical_mask)
            interpolated = True
        # if not 
        else:
            new_sample = self.add_gaussian_noise(x=self.data_numpy[reference_sample_index],
                                                                    standard_deviations= self.standard_deviations,
                                                                    noise_factor= self.noise_factor,
                                                                    n_samples= 1,
                                                                    numerical_mask= self.numerical_mask,
                                                                    categorical_mask=self.categorical_mask
                                                                    )
            interpolated = False
 
        return new_sample, interpolated